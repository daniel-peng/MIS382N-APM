{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 3</p>\n",
    "## <p style=\"text-align: center;\">Total points: 80 </p>\n",
    "## <p style=\"text-align: center;\">Due: Mon, October 24, by 11:59pm</p>\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. Please submit **only one** ipynb file from each group, and include the names of all the group members. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Stochastic Gradient Descent (10pts)\n",
    "\n",
    "1. Using stochastic gradient descent, derive the coefficent updates for all 4 coefficients of the model: $$ y = w_0 + w_1*x_1 + w_2*x_1^2 + w_3*x_2 $$ Hint: start from the cost function (Assume sum of squared error). If you write the math by hand, submit that as a separate file and make a reference to it in your notebook or include the image in your notebook.\n",
    "2. Write Python code for an SGD solution to the non-linear model $$ y = w_0 + w_1*x_1 + w_2*x_1^2 + w_3*x_2$$ Try to format similarly to scikit-learn's models. There should be a _fit_ function that takes parameters X, y, learning rate, and number of iterations, and a _predict_ function that takes an X value (optionally, an array of values). Use your new gradient descent regression to predict the data given in 'samples.csv', for 10 epochs, using learning rates: [.0001, .001, .01] . Plot MSE and the $w$ parameters as a function of epoch count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Gradient Descent (5 pts)\n",
    "\n",
    "Suppose we are trying to use gradient descent to minimize a cost function y = f(w) as shown in the figure below. This function is linearly decreasing between A and B, constant between B and C, quadratic between C and D and constant between D and E. Assume that we have 10000 data points in our training set. If we choose the starting point between B and C, will we be able to find the local minima? Explain your answer. If your answer is \"Yes\", can you give a bound on the number of iterations required to get to the local minima?\n",
    "\n",
    "<img src=\"sgd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multi-layer Perceptron regressor (15 points)\n",
    "\n",
    "In this question, you will explore the application of Multi-layer Perceptron (MLP) regression using sklearn package in Python. We will use the same dataset used in HW2 Q5: Hitters.csv [here](https://rdrr.io/cran/ISLR/man/Hitters.html). \n",
    "\n",
    "Following code will load and split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 42** and **test_size = 0.33**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjor/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('Hitters.csv')\n",
    "label_name = 'Salary'\n",
    "y = data[label_name]\n",
    "X = data.drop(label_name,axis=1)\n",
    "print X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to use in this problem is [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on original data, use StandardScaler to make each feature centered ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Whenever you have training and test data, fit a scaler on training data and use this scaler on test data. Here, scale only features (independent variables), not target variable y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use [sklearn.neural_nework.MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) to do a 5-fold cross validation using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). The cross validation must be performed on the **training data**. Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=40, learning_rate_init = 0.001\n",
    "    \n",
    "Now, consider two different settings for the number of hidden units:\n",
    "    \n",
    "   (a) *hidden_layer_sizes = (2,)* (b) *hidden_layer_sizes = (15,)*\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model: (a) and (b) (6pts)\n",
    "   \n",
    "   \n",
    "2) Now, using the same parameters used in part 1), train MLPRegressor models on whole training data and report RMSE score for both Train and Test set (Again, use StandardScaler). Which model works better, (a) or (b)? Briefly analyze the result in terms of the number of hidden units. (5pts)\n",
    "\n",
    "\n",
    "3) MLPRegressor has a built-in attribute *loss\\_curve\\_* which returns the loss at each iteration. For example, if your model is named as *my_model* you can call it as *my\\_model.loss\\_curve\\_* ([example](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py)). Plot two curves for model (a) and (b) in one figure, where *X-axis* is iteration number and *Y-axis* is squared root of *loss\\_curve\\_* value. (4pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Bayesian Classifiers (10 pts)\n",
    "\n",
    "Download the Smarket dataset from Canvas. This contains about four years worth of daily prices for one stock. The goal is to predict whether or not the stock price will go up or down, and the features are the stock prices of the last five days.  \n",
    "The code below loads the dataset and all necessary sklearn modules (not that you can't use more if you feel like it). Look up any module on the scikit-learn website for a full description.\n",
    "\n",
    "1. The last 50 points will be the test dataset. For training, use the 1000 points prior to these 50 test points.\n",
    "2. Train Linear Discriminant Analysis, Quadratic Discriminant Analysis, and (Gaussian) Naive Bayes. Extract the probability of the stock price going up for each row in the test set.\n",
    "3. Plot the receiver operating characteristic (ROC) curve of each model, using the extracted probabilities and the true values for the test set. (3 pts)\n",
    "4. Report the area under the ROC curve (AUC) for each model. (2 pts)\n",
    "6. Justify the performance of each model, relative to the others. (1 pts)\n",
    "7. Repeat steps 1-6, only using the prior 100 points for training. Explain the changes in model performance. (4 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "data = pd.read_csv('Smarket.csv', usecols=['Lag1','Lag2','Lag3','Lag4','Lag5','Direction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 - Logistic Regression (15pts)\n",
    "\n",
    "In this question we will be predicting mile per gallon (mpg) for Auto data set. ('Auto.csv' in Canvas)\n",
    "1. Convert mpg to a binary variable mpg01 which is 1 if had an mpg is greater than median mpg and zero otherwise\n",
    "2. Split the data into training and test. Use 42 as random seed and use 1/3rd of the data for testing. Our y variable is mpg01 and X matrix includes all the other variables except mpg01.\n",
    "3. Train a logistic regression with almost no regularization (pass l2 (ridge) to penalty and 1,000,000 to the C parameter which is the inverse of regularization strength lambda. This essentially does l2 regularization but applies very little weight to the penalty term) and report the [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) on the test data. Also report the accuracy for the \"mpg01 = 0\" class, the \"mpg01 = 1\" class, and the average per-class accuracy on the test data. Average per-class accuracy is described in this [post](http://rasbt.github.io/mlxtend/user_guide/evaluate/scoring/). You can use your confusion matrix to calculate this.\n",
    "4. Repeat step 3 except use l2 penalty with Cs of [0.001,0.01, 0.1, 1, 10 ,100, 1000]. You will want to use k-fold cross validation to select the best parameter. To evaluate which parameter is best, maximize the average per-class accuracy. To help with this task, check out [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and how to make your own [custom scorer](http://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "5. Repeat question 4 except use l1 instead of l2 as the penalty type, use Cs of  [0.001, 0.01, ..., 1000]\n",
    "6. Which model produces the best average per-class accuracy? Why do you think this is the case? How do the models handle the different classes, and why is this so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code will load and clean the dataset and load some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0    70   \n",
       "1  15.0          8         350.0       165.0    3693          11.5    70   \n",
       "2  18.0          8         318.0       150.0    3436          11.0    70   \n",
       "3  16.0          8         304.0       150.0    3433          12.0    70   \n",
       "4  17.0          8         302.0       140.0    3449          10.5    70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn import cross_validation\n",
    "# from sklearn import model_selection # Use model_selection instead of cross_validation in sklearn version >=0.18\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "Auto = pd.read_csv('Auto.csv', na_values='?').drop('name',axis = 1).dropna()\n",
    "Auto.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1\n",
    "mpg_median = Auto['mpg'].median()\n",
    "Auto['mpg01'] = 1\n",
    "Auto.loc[Auto.mpg <= mpg_median,'mpg01'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2\n",
    "Y, X = dmatrices('mpg01 ~ 0+cylinders+displacement+horsepower+weight+acceleration+year+origin',\n",
    "                 Auto,return_type=\"dataframe\")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1.0/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 15]\n",
      " [ 2 59]]\n",
      "mpg01 = 0 accuracy: 0.8\n",
      "mpg01 = 1 accuracy: 0.96\n",
      "average per-class accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "mpg_logistic = LogisticRegression(penalty = 'l2',C = 1000000)\n",
    "mpg_logistic.fit(X_train,Y_train)\n",
    "y_pred_logistric = mpg_logistic.predict(X_test)\n",
    "cm = confusion_matrix(Y_test,y_pred_logistric)\n",
    "tn = float(cm[1][1])\n",
    "tp = float(cm[0][0])\n",
    "fn = float(cm[0][1])\n",
    "fp = float(cm[1][0])\n",
    "print cm\n",
    "print 'mpg01 = 0 accuracy:', round(tn/(tn+fn),2)\n",
    "print 'mpg01 = 1 accuracy:', round(tp/(tp+fp),2)\n",
    "print 'average per-class accuracy:', round((tp+tn)/len(Y_test),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01}\n",
      "0.920153256705\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "def per_class_accuracy(ground_truth, prediction):\n",
    "    cm = confusion_matrix(ground_truth,prediction)\n",
    "    tn = float(cm[1][1])\n",
    "    tp = float(cm[0][0])\n",
    "    return round((tp+tn)/len(ground_truth),2)\n",
    "\n",
    "accuracy_score = make_scorer(per_class_accuracy, greater_is_better=True)\n",
    "parameters = {'C':[0.001,0.01,0.1, 1, 10 ,100, 1000]}\n",
    "clf = GridSearchCV(mpg_logistic, parameters, cv = 5, scoring= accuracy_score)\n",
    "clf.fit(X_train,Y_train.mpg01.values)\n",
    "print clf.best_params_\n",
    "print clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.924137931034\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "mpg_logistic_l1 = LogisticRegression(penalty = 'l1',C = 1000000)\n",
    "clf_l1 = GridSearchCV(mpg_logistic_l1, parameters, cv = 5, scoring= accuracy_score)\n",
    "clf_l1.fit(X_train,Y_train.mpg01.values)\n",
    "print clf_l1.best_params_\n",
    "print clf_l1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56 14]\n",
      " [ 1 60]]\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "best_mpg = LogisticRegression(penalty = 'l1',C = 10)\n",
    "best_mpg.fit(X_train,Y_train)\n",
    "y_best_mpg = best_mpg.predict(X_test)\n",
    "cm = confusion_matrix(Y_test,y_best_mpg)\n",
    "print cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is the one with l1 penalty and C = 10. This is because l1 and C of 10 make the coefficients shrink faster, leaving only the important variables. A high C can drive down both true positives and true negatives, because when the penalty is low the model tends to overfit the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 6: House Prices (kaggle competition) (25 pts)\n",
    "\n",
    "In this problem, we are going to explore a kaggle competition: [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Your goal is to obtain the best score you can in this competition. This is an ongoing competition, and you have the opportunity to win the prize money! \n",
    "\n",
    "The first step is to make a Kaggle account. Then find the House Prices competition and read the competition details and the description of the dataset. You may find this [article](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf) useful.\n",
    "\n",
    "Your work should meet the following requirements:\n",
    "\n",
    "1. Data Preprocessing. \n",
    " * Conduct some data preprocessing. (Hint: see if there is any skewed features and consider applying suitable transformation techniques to make them more \"normal\").\n",
    " * Impute the missing values (if any).\n",
    "2. Predictive Models. \n",
    " * You have to create at least three models: simple linear regression, Lasso and Ridge regression and multilayer perceptron. You may consider creating an ensemble of these models as well (optional). For Lasso and Ridge regression, optimize the alphas using cross validation. You may try other predictive models to get better scores (optional).\n",
    "3. Evaluation: submit your model to kaggle submission site and report the public score.\n",
    "\n",
    "Briefly describe your work on each of these steps. Explain (very briefly) what approaches you tried, what worked and what did not work. Mention your team's kaggle name and include a screen shot of your public submission score. Finally, try your best to win this competition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "house = pd.read_csv('train.csv')\n",
    "house_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#impute with mean\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "\n",
    "numerical = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', \n",
    "            'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "            'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n",
    "            '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n",
    "            'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "             'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',\n",
    "            'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "            'MiscVal', 'MoSold', 'YrSold']\n",
    "for var in numerical:\n",
    "    if house[var].isnull().values.any():\n",
    "        mean = house[var].mean()\n",
    "        house.loc[house[var].isnull(), var] = mean\n",
    "    if house_test[var].isnull().values.any():\n",
    "        mean_test = house_test[var].mean()\n",
    "        house_test.loc[house_test[var].isnull(), var] = mean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create dummy variables\n",
    "new_house = house\n",
    "new_house_test = house_test\n",
    "categorical = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n",
    "               'LandContour','Utilities','LotConfig','LandSlope',\n",
    "              'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "              'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "              'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', \n",
    "              'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "              'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\n",
    "              'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n",
    "              'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
    "              'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "              'SaleType', 'SaleCondition']\n",
    "\n",
    "for c in categorical:\n",
    "    dummy = pd.get_dummies(house[c], prefix=c)\n",
    "    new_house = pd.concat([new_house, dummy], axis=1)\n",
    "    new_house.drop([c,dummy.columns.values[0]],inplace=True, axis=1)\n",
    "    \n",
    "    dummy_test = pd.get_dummies(house_test[c], prefix=c)\n",
    "    new_house_test = pd.concat([new_house_test, dummy_test], axis=1)\n",
    "    new_house_test.drop([c,dummy_test.columns.values[0]],inplace=True, axis=1)\n",
    "     \n",
    "train_Y = new_house.SalePrice\n",
    "train_X = new_house.drop('SalePrice', axis=1)\n",
    "test_X = new_house_test\n",
    "\n",
    "combine = pd.concat([train_X,test_X])\n",
    "combine = combine.fillna(0)\n",
    "train_X = combine[:len(train_X)]\n",
    "test_X = combine[len(train_X):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423351748.074\n"
     ]
    }
   ],
   "source": [
    "#simple linear regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(train_X, train_Y)\n",
    "test_Y = regr.predict(test_X)\n",
    "print mean_squared_error(train_Y,regr.predict(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666711093.456\n"
     ]
    }
   ],
   "source": [
    "#Ridge\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "ridge_model = linear_model.Ridge()\n",
    "accuracy_score = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "parameters = {'alpha':10**np.linspace(10,-2,100)*0.5}\n",
    "clf = GridSearchCV(ridge_model, parameters, cv = 5, scoring= accuracy_score)\n",
    "clf.fit(train_X,train_Y)\n",
    "alpha_ridge = clf.best_params_['alpha']\n",
    "\n",
    "best_ridge = linear_model.Ridge(alpha = alpha_ridge)\n",
    "best_ridge.fit(train_X,train_Y)\n",
    "prediction_ridge = best_ridge.predict(test_X)\n",
    "print mean_squared_error(train_Y, best_ridge.predict(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjor/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:479: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652868724.251\n"
     ]
    }
   ],
   "source": [
    "#Lasso\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "lasso_model = linear_model.Lasso()\n",
    "accuracy_score = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "parameters = {'alpha':10**np.linspace(10,-2,100)*0.5}\n",
    "clf = GridSearchCV(lasso_model, parameters, cv = 5, scoring= accuracy_score)\n",
    "clf.fit(train_X,train_Y)\n",
    "alpha_lasso = clf.best_params_['alpha']\n",
    "\n",
    "best_lasso = linear_model.Lasso(alpha = alpha_lasso)\n",
    "best_lasso.fit(train_X,train_Y)\n",
    "prediction_lasso = best_lasso.predict(test_X)\n",
    "print mean_squared_error(train_Y, best_lasso.predict(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2277918751.2553024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(50,))\n",
    "mlp.fit(train_X,train_Y)\n",
    "prediction_mlp = mlp.predict(test_X)\n",
    "mean_squared_error(train_Y,mlp.predict(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output\n",
    "output = prediction_mlp\n",
    "final = pd.DataFrame(output, index=new_house_test.Id.values, columns=['SalePrice'])\n",
    "final.index.name = 'Id'\n",
    "final.to_csv('prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
