{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 2</p>\n",
    "## <p style=\"text-align: center;\">Total points: 60</p>\n",
    "## <p style=\"text-align: center;\">Due: Wed, October 05, by 11:59pm</p>\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook** (except for Q6). Please submit **only one** ipynb file from each group, and include the names of all the group members in your ipynb file. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting.\n",
    "\n",
    "**Team Members: Billy Yuan, Lindsay Tober**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 1: Sampling (6 pts)\n",
    "\n",
    "A recent survey estimated that $30\\%$ of all Europeans aged 20 to 22 have driven under the influence of drugs or alcohol, based on a simple \"Yes or No\" question. A similar survey is being planned for Americans. The survey designers want the  $90\\%$ confidence interval to have a margin of error of at most $\\pm0.09$.\n",
    "\n",
    "(a) Find the necessary sample size needed to conduct this survey assuming that the expected percentage of \"yes\" answers will be very close to that obtained from the European survey? (2 pts)\n",
    "\n",
    "(b) Suppose the tolerance level was kept the same but the confidence level needs to increase to $95\\%$. What is the required sample size for this new specification? (2 pts)\n",
    "\n",
    "(c) If one does not know where the true \"$p$\" may lie, one can conservatively conduct a survey assuming the worst case (in terms of required minimum sample size)  scenario of  $p = 0.5$. Redo part (b) for this \"worst case\" scenario. (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an expected percentage of \"yes\" equal to $30\\%$, a desired confidence interval of $90\\%$, and a desired margin of error of at most $\\pm 0.09$, we can determine the necessary sample size by the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{n} & \\ge {p(1-p) * \\left( {\\frac{z_{\\alpha / 2}}{\\epsilon}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.3(1-0.3) * \\left( {\\frac{z_{0.1 / 2}}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.3(0.7) * \\left( {\\frac{z_{0.05}}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.21 * \\left( {\\frac{1.645}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "{n} & \\ge {70}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tolerance level is kept the same but the confidence level is increased to $95\\%$, we can recalculated the required sample size as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{n} & \\ge {p(1-p) * \\left( {\\frac{z_{\\alpha / 2}}{\\epsilon}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.3(1-0.3) * \\left( {\\frac{z_{0.05 / 2}}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.3(0.7) * \\left( {\\frac{z_{0.025}}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.21 * \\left( {\\frac{1.96}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "{n} & \\ge {100}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not know where the true \"$p$\" may lie, we can conservatively estimate $n$ by assuming the worst case (in terms of required minimum sample size) scenario of $p=0.5$:\n",
    " \n",
    " \n",
    "$$\n",
    "\\begin{align}\n",
    "{n} & \\ge {p(1-p) * \\left( {\\frac{z_{\\alpha / 2}}{\\epsilon}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.5(1-0.5) * \\left( {\\frac{z_{0.05 / 2}}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.5(0.5) * \\left( {\\frac{z_{0.025}}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "& \\ge {0.25 * \\left( {\\frac{1.96}{0.09}} \\right)^2 } \\\\\n",
    "\\\\\n",
    "{n} & \\ge {119}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (2+2 = 4 points)\n",
    "\n",
    "View the video at:\n",
    "\n",
    "https://www.youtube.com/watch?v=jbkSRLYSojo\n",
    "\n",
    "(Hans Rosling's 200 Countries, 200 Years, 4 Minutes) and answer the following questions:\n",
    "\n",
    "1. How many variables are being visualized in the “moving bubble plots” video (list them)?\n",
    "\n",
    "2. Identify a variable that is “zoomed into”, i.e., examined at a sub-category or more detailed level.\n",
    "\n",
    "\n",
    "FACTOID: Rosling’s gapminder visualization\n",
    "\n",
    "(see https://www.youtube.com/user/Gapcast for some more insightful videos) can now be\n",
    "\n",
    "readily used by you via Google Charts: https://developers.google.com/chart/interactive/docs/gallery\n",
    "\n",
    "Just plug in your own variables into “Bubble Chart” under the URL above and go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Principal Component Analysis (PCA) (10 pts)\n",
    "\n",
    "Download the US imports dataset from Canvas, or from [here](https://www.census.gov/foreign-trade/statistics/product/enduse/imports/enduse_imports.xlsx).\n",
    "\n",
    "This code will clean the data and format it so that it is PCA-ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicated countries: \n",
      "['Montenegro', 'Serbia', 'Sudan']\n",
      "countries that don't trade with USA: \n",
      "['Cuba', 'Korea, North', 'Netherlands Antilles']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('enduse_imports.xlsx')\n",
    "\n",
    "# gather columns we care about\n",
    "df = df.loc[:,['CTY_DESC','COMM_DESC','value_15']]\n",
    "df.columns = pd.Series(['Country','Good','Value'])\n",
    "\n",
    "# not a country, remove\n",
    "df = df[df['Country'] != 'World Total']\n",
    "\n",
    "# some countries had imports recorded twice\n",
    "# remove these countries for simplicity\n",
    "importCount = df.groupby(['Country','Good']).count().iloc[:,0]\n",
    "duplicatedImports = importCount[importCount > 1]\n",
    "countriesWithDuplicates = duplicatedImports.index.get_level_values(0).unique()\n",
    "print \"duplicated countries: \"\n",
    "print [str(country) for country in countriesWithDuplicates]\n",
    "df = df[df['Country'].isin(countriesWithDuplicates) == False]\n",
    "\n",
    "# remove countries that don't export\n",
    "totalImports = df[['Country','Value']].groupby('Country').sum().iloc[:,0]\n",
    "countriesWithNoImport = totalImports[totalImports == 0].index\n",
    "print \"countries that don't trade with USA: \"\n",
    "print [str(country) for country in countriesWithNoImport]\n",
    "df = df[df['Country'].isin(countriesWithNoImport) == False]\n",
    "\n",
    "# reshape so that each type of good has its own column\n",
    "df = df.pivot(index='Country',columns='Good',values='Value')\n",
    "df = df.fillna(0)\n",
    "\n",
    "# import PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Good</th>\n",
       "      <th>Agricultural machinery, equipment</th>\n",
       "      <th>Alcoholic beverages, excluding wine</th>\n",
       "      <th>Apparel, household goods - cotton</th>\n",
       "      <th>Apparel, household goods - wool</th>\n",
       "      <th>Apparel, textiles, nonwool or cotton</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>3105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10739.0</td>\n",
       "      <td>7314.0</td>\n",
       "      <td>11942.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34741.0</td>\n",
       "      <td>2752171.0</td>\n",
       "      <td>50838.0</td>\n",
       "      <td>1298224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andorra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24505.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Good         Agricultural machinery, equipment  \\\n",
       "Country                                          \n",
       "Afghanistan                             3105.0   \n",
       "Albania                                    0.0   \n",
       "Algeria                                    0.0   \n",
       "Andorra                                    0.0   \n",
       "Angola                                     0.0   \n",
       "\n",
       "Good         Alcoholic beverages, excluding wine  \\\n",
       "Country                                            \n",
       "Afghanistan                                  0.0   \n",
       "Albania                                  34741.0   \n",
       "Algeria                                      0.0   \n",
       "Andorra                                      0.0   \n",
       "Angola                                   24505.0   \n",
       "\n",
       "Good         Apparel, household goods - cotton  \\\n",
       "Country                                          \n",
       "Afghanistan                            10739.0   \n",
       "Albania                              2752171.0   \n",
       "Algeria                                    0.0   \n",
       "Andorra                                  351.0   \n",
       "Angola                                     0.0   \n",
       "\n",
       "Good         Apparel, household goods - wool  \\\n",
       "Country                                        \n",
       "Afghanistan                           7314.0   \n",
       "Albania                              50838.0   \n",
       "Algeria                                  0.0   \n",
       "Andorra                                  0.0   \n",
       "Angola                                   0.0   \n",
       "\n",
       "Good         Apparel, textiles, nonwool or cotton  \n",
       "Country                                            \n",
       "Afghanistan                               11942.0  \n",
       "Albania                                 1298224.0  \n",
       "Algeria                                       0.0  \n",
       "Andorra                                       0.0  \n",
       "Angola                                        0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5,:5] # display first five rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now gather the top two principal components from this dataset and  \n",
    "(a) Make a scatter plot with the first component as the x-axis and the second as the y-axis. (3 pts)\n",
    "\n",
    "(b) Find the names of the six countries with the highest first component (these should be clear outliers). (2 pts)\n",
    "\n",
    "(c) Given the results of parts (a) and (b), one might theorize that the first component roughly represents the total volume of exports to the US.  Using the components\\_ attribute, gather the loadings of the first component.  Also use the original dataframe to gather the total imports to the US for each good.  Divide this list of total imports per good by the total US imports period, so that for each good we know what percent of imports it accounted for.  Make a scatter plot with this value on the x-axis and the first component's loadings on the right. (3 pts)\n",
    "\n",
    "(d) For the four goods with the highest component loadings, print the name of the good and the percent of imports it accounts for.  Briefly comment on whether you think the first component represents the total volume of imports, or whether it has another interpretation - no right or wrong answer. (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-29f6a4961200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mpc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpca_transform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# country names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mcountries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcountry\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcountry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_norm' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "# Pretty colors    \n",
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)] \n",
    "  \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)  \n",
    "\n",
    "\n",
    "# fit PCA to data using 2 components. Each row will be a feature and \n",
    "# each column will be a principal component\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(df)\n",
    "# transform original data so that each row is a country and each column is a principal component\n",
    "pca_transform = pca.transform(df)\n",
    "# store the first component into X list\n",
    "pc1 = [item[0] for item in pca_transform]\n",
    "# store the second component into Y list\n",
    "pc2 = [item[1] for item in pca_transform]\n",
    "# country names\n",
    "countries = [country for country in df_norm.index.values]\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.scatter(pc1, pc2, color=tableau20[0])\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(True)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(True) \n",
    "\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()  \n",
    "\n",
    "plt.title(\"PCA Components 1 and 2\", fontsize = 16)\n",
    "plt.xlabel(\"PC1\", fontsize = 13)\n",
    "plt.ylabel(\"PC2\", fontsize = 13)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 6 outliers are:\n",
    "```\n",
    "1. China\n",
    "2. Mexico\n",
    "3. Canada\n",
    "4. Japan\n",
    "5. Germany\n",
    "6. South Korea\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign index to country using enumerate and sort by the score of each country\n",
    "x_index = sorted(list(enumerate(pc1)),key=lambda tup: tup[1],reverse=True)\n",
    "\n",
    "for index, item in x_index[:6]:\n",
    "    print countries[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "We plotted the percentage of total U.S. imports by the loading of PC1. The percentages of total U.S. imports for each good did not use standardized numbers in order to observe the effect of the true percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadings_pc1 = pca.components_[0]\n",
    "import_perc_df = 100*df.apply(sum) / sum(df.apply(sum))\n",
    "\n",
    "plt.figure(figsize=(8, 6)) \n",
    "\n",
    "plt.scatter(import_perc_df,loadings_pc1, color=tableau20[0])\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(True)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(True) \n",
    "\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()  \n",
    "\n",
    "ax.set_xlim(-1,8)\n",
    "\n",
    "plt.title(\"Percent of Total U.S. Imports vs. PC1 Loading\", fontsize = 16)\n",
    "plt.xlabel(\"Percent of Total US Imports (%)\", fontsize = 13)\n",
    "plt.ylabel(\"PC1 Loading\", fontsize = 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "The 4 groups with the highest loadings in PC1 (the number is the percent of US imports the group accounts for) are:\n",
    "\n",
    "```\n",
    "Cell phones and other household goods, n.e.c.: Loading - 0.5, % Imports - 4.39%\n",
    "Computers: Loading - 0.37, % Imports - 2.81%\n",
    "Other parts and accessories of vehicles: Loading - 0.31, % Imports - 4.73%\n",
    "Passenger cars, new and used: Loading - 0.27, % Imports - 7.39%\n",
    "```\n",
    "\n",
    "We believe that PC1 may represent electronic and consumer products. The correlation between the percent of total US imports and loading from PC1 is ```0.797```, which suggests that the component partially represents the goods that have the highest volume of trading. However, because the correlation among the points is lower for goods with volume above 4%, volume cannot be the only explanation for PC1's loadings. What the top 4 goods with the highest loadings of PC1 have in common is that they are all electronic or consumer products. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_5_pc1 = sorted(list(enumerate(loadings_pc1)), key=lambda tup:tup[1], reverse=True)[:4]\n",
    "goods = [item for item in df_norm.columns.values]\n",
    "\n",
    "for key, item in top_5_pc1:\n",
    "    print \"{}: Loading - {}, % Imports - {}%\".format(goods[key], round(item,2), round(import_perc_df.iloc[key],2))\n",
    "    \n",
    "print \"\\n\",\\\n",
    "    \"Correlation between % of total imports and PC1 weights: {:.3f}\".format\\\n",
    "    (np.corrcoef(import_perc_df,loadings_pc1)[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Visualization using Bokeh (10 pts)\n",
    "\n",
    "In this problem, you'll build an interactive visualization. Bokeh is a Python interactive visualization library that targets modern web browsers for presentation. For more information on Bokeh, see http://bokeh.pydata.org/en/latest/. The problem statement is as follows:\n",
    "\n",
    "Using the [King County House Sales](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original) data, your goal is to build a Bokeh visualization which allows the user explore how price varies with living room size and year built. You will create a visualization that allows the user to toggle the X axis of a scatter plot between living room size and year built, with the y-axis always being price. Also add the hover tool so that if the user hovers over a datapoint in the living-room-size plot a window pops up that shows year built - and vice versa.\n",
    "\n",
    "Hints: \n",
    "1. You can make use of Select widgets.\n",
    "2. See: http://bokeh.pydata.org/en/latest/docs/user_guide/interaction.html#javascript-callbacks. Specifically look at the CustomJS for Widgets under Callbacks and the Select widget. \n",
    "3. see: http://bokeh.pydata.org/en/latest/docs/user_guide/tools.html#basic-tooltips for a hover tool example.\n",
    "4. See: http://bokeh.pydata.org/en/latest/docs/reference/plotting.html. Look for the scatter API.\n",
    "5. See: http://bokeh.pydata.org/en/0.10.0/docs/user_guide/styling.html#labels. For labeling axes.\n",
    "6. Use output_notebook() from Bokeh to output the plot to your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bokeh.plotting import figure, output_file, output_notebook, show, ColumnDataSource\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.models import HoverTool, NumeralTickFormatter\n",
    "\n",
    "kc_data = pd.read_csv(\"kc_house_data.csv\")\n",
    "\n",
    "# Create dataframe of price, sqft_living, and yr_built\n",
    "kc_data_bokeh = kc_data[['price','sqft_living','yr_built']]\n",
    "X1 = list(kc_data['sqft_living']/1000)\n",
    "X2 = list(kc_data['yr_built'])\n",
    "Y = list(kc_data['price']/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "output_file(\"Question 4.html\")\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            sqft=X1,\n",
    "            Y_=Y,\n",
    "            yr_built=X2,\n",
    "        )\n",
    "    )\n",
    "\n",
    "hover1 = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"Year Built:\", \"@yr_built\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "hover2 = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"Sq. Ft:\",\"@sqft\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p1 = figure(width=300, height=300,tools=[hover1],title = \"Sq. Feet vs. Price\")\n",
    "p2 = figure(width=300, height=300,tools=[hover2],title=\"Year Built vs. Price\")\n",
    "\n",
    "p1.scatter(x='sqft',y='Y_', source=source)\n",
    "p2.scatter(x='yr_built',y='Y_', source=source)\n",
    "\n",
    "p1.xaxis.axis_label = \"Sq. Feet (in '000)\"\n",
    "p1.yaxis.axis_label = \"Price (in MM)\"\n",
    "\n",
    "p2.xaxis.axis_label = \"Year Built\"\n",
    "p2.yaxis.axis_label = \"Price (in MM)\"\n",
    "\n",
    "p1.yaxis[0].formatter = NumeralTickFormatter(format=\"0\")\n",
    "\n",
    "tab1 = Panel(child=p1, title = 'Sq. Feet of Living Room')\n",
    "tab2 = Panel(child=p2, title=\"Year Built\")\n",
    "\n",
    "tabs = Tabs(tabs=[ tab1, tab2 ])\n",
    "\n",
    "show(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 5: Ridge and Lasso Regression (6+6+4+4=20 points)\n",
    "\n",
    "In this question, you will explore the application of Lasso and Ridge regression using sklearn package in Python. The dataset is Hitters.csv (available on canvas), which contains performance records and salaries for baseball players. More information on the data can be found [here](https://rdrr.io/cran/ISLR/man/Hitters.html). There are 17 variables: first 16 columns are performance related features and the last column is for Salary. We\n",
    "wish to predict a baseball player’s Salary using all the 16 performance variables. Use a random state of 42 and a test size of 1/3 to [split the data into training and test](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html). Note: lambda is called alpha in sklearn.\n",
    "\n",
    "1. Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html). For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{10}$ to $\\lambda = 10^{-2}$. In Python, you can consider this range of values as follows:\n",
    "```\n",
    "    import numpy as np\n",
    "\n",
    "    alphas =  10\\***np.linspace(10,-2,100)*\\*0.5\n",
    "```\n",
    "    Report the best chosen $\\lambda$ based on cross validation. The cross validation should happen on your training data using  average MSE as the scoring metric.\n",
    "2. Run ridge and lasso for all of the parameters specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed? \n",
    "3. Run least squares regression, ridge, and lasso on the training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error (MSE) on the test data for each.\n",
    "4. Run lasso again with cross validation using [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Set the cross validation parameters as follows:\n",
    "```\n",
    "    LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "```\n",
    "    Report the best $\\lambda$ based on cross validation. Run lasso on the training data using the best $\\lambda$ and report the coefficeints for 16 variables. What do you observe from these coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "Based on 5-fold cross-validation, the best $\\lambda$ values are:\n",
    "\n",
    "```\n",
    "Best Ridge lambda: 1424.018\n",
    "Best LASSO lambda: 28.612\n",
    "```\n",
    "\n",
    "Our steps were:\n",
    "1. Split data into training-test (70-30 split)\n",
    "2. Perform 5-fold CV on training set for each value of $\\lambda$ between $10^{-2}$ to $10^{10}$\n",
    "3. Record the average RMSE of 5-fold CV for each $\\lambda$\n",
    "4. The $\\lambda$ with the lowest average RMSE is the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import Lasso,LassoCV,Ridge, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "hitters = pd.read_csv(\"Hitters.csv\")\n",
    "\n",
    "# Get indexes of predictor variables\n",
    "columns = list(enumerate(hitters.columns.values))\n",
    "predictor_index = [tup[0] for tup in columns if tup[1] != 'Salary']\n",
    "\n",
    "# Split target and predictors\n",
    "target = hitters['Salary']\n",
    "predictors = hitters.iloc[:,predictor_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split test and training data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(predictors, target, test_size=0.3, random_state=42)\n",
    "\n",
    "alphas =  10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "ridge = Ridge(random_state=42)\n",
    "lasso = Lasso(random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42)\n",
    "kf.get_n_splits(X_train)\n",
    "\n",
    "# mse helper\n",
    "def mse_calc(train, test):\n",
    "    return (np.mean((train-test)**2))\n",
    "\n",
    "# Ridge - calculate RMSE for each fold and each alpha to determine best lambda\n",
    "ridge_score = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    cv_ridge = []\n",
    "    for train,test in kf.split(X_train):\n",
    "\n",
    "        ridge = Ridge(random_state=42, alpha=alpha)\n",
    "        ridge.fit(X_train.iloc[train], Y_train.iloc[train])\n",
    "\n",
    "        yhat_ridge = ridge.predict(predictors.iloc[test])\n",
    "        y_actual = target[test]\n",
    "\n",
    "        cv_ridge.append(mse_calc(yhat_ridge,y_actual))\n",
    "    \n",
    "    ridge_score.append((alpha,np.mean(cv_ridge)))\n",
    "    \n",
    "# LASSO - calculate RMSE for each fold and each alpha to determine best lambda\n",
    "lasso_score = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    cv_lasso = []\n",
    "    for train,test in kf.split(X_train):\n",
    "\n",
    "        lasso = Lasso(random_state=42, alpha=alpha)\n",
    "        lasso.fit(X_train.iloc[train], Y_train.iloc[train])\n",
    "\n",
    "        yhat_lasso = lasso.predict(predictors.iloc[test])\n",
    "        y_actual = target[test]\n",
    "\n",
    "        cv_lasso.append(mse_calc(yhat_lasso,y_actual))\n",
    "    \n",
    "    lasso_score.append((alpha,np.mean(cv_lasso)))\n",
    "    \n",
    "# Store best values of lambda for Ridge and LASSO based on the lowest RMSE\n",
    "best_lambda_ridge = sorted(ridge_score, key = lambda x: x[1])[0][0]\n",
    "best_lambda_lasso = sorted(lasso_score, key = lambda x: x[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Best Ridge lambda: {:.3f}\".format(best_lambda_ridge)\n",
    "print \"Best LASSO lambda: {:.3f}\".format(best_lambda_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "For both Ridge and Lasso, when the regularization parameter $\\lambda$ increases, the value of the coefficients decrease. Some coefficients decrease faster than others. However, in Ridge, the coefficients approach 0 as $\\lambda$ increases from $10^4$ to $10^{10}$. In Lasso, the coefficients begin to become zero from $\\lambda = 10^2$.\n",
    "\n",
    "The values of $\\lambda$ were not calculated using 5-fold CV here. We just calculated the coefficients from fitting the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store coefficients for ridge and lasso in lists. These will be our y-values in the plots\n",
    "coef_ridge = []\n",
    "coef_lasso = []\n",
    "\n",
    "for alpha in alphas:\n",
    "\n",
    "    # Calculate coefficients for ridge \n",
    "    ridge = Ridge(random_state=42, alpha=alpha)\n",
    "    ridge.fit(X_train, Y_train)\n",
    "        \n",
    "    coef_ridge.append(ridge.coef_)\n",
    "\n",
    "    # Calculate coefficients for lasso\n",
    "    lasso = Lasso(random_state=42, alpha=alpha)\n",
    "    lasso.fit(X_train, Y_train)\n",
    "        \n",
    "    coef_lasso.append(lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,12))\n",
    "\n",
    "# Ridge\n",
    "plt.subplot(211)\n",
    "\n",
    "plt.plot(alphas, coef_ridge)\n",
    "plt.xscale('log')\n",
    "plt.xlim(ax1.get_xlim()[::-1])  # reverse axis\n",
    "\n",
    "plt.title(\"Coefficients vs. Lambda in Ridge\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Coefficient Weights\")\n",
    "\n",
    "# Lasso\n",
    "plt.subplot(212)\n",
    "\n",
    "plt.plot(alphas, coef_lasso)\n",
    "plt.xscale('log')\n",
    "plt.xlim(ax2.get_xlim()[::-1]) \n",
    "\n",
    "plt.title(\"Coefficients vs. Lambda in LASSO\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Coefficient Weights\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Here are the MSE results from OLS, Ridge, and LASSO regressions using only the training data:\n",
    "\n",
    "```\n",
    "OLS Test Error: 150074.549\n",
    "LASSO Test Error: 145946.110\n",
    "Ridge Test Error: 144860.636\n",
    "```\n",
    "\n",
    "We used the best values of lambda calculated in Parts 1 and 2. Our lowest MSE came from the Ridge model, followed by the LASSO model. The OLS model performed the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear = LinearRegression()\n",
    "ridge_3 = Ridge(random_state=42, alpha=best_lambda_ridge)\n",
    "lasso_3 = Lasso(random_state=42, alpha=best_lambda_lasso)\n",
    "\n",
    "# Fit linear model to training data\n",
    "linear.fit(X_train, Y_train)\n",
    "yhat_linear = linear.predict(X_test)\n",
    "\n",
    "# Fit Ridge model to training data\n",
    "ridge_3.fit(X_train, Y_train)\n",
    "yhat_ridge = ridge_3.predict(X_test)\n",
    "\n",
    "# Fit LASSO model to training data\n",
    "lasso_3.fit(X_train, Y_train)\n",
    "yhat_lasso = lasso_3.predict(X_test)\n",
    "\n",
    "print \"OLS Test MSE: {:.3f}\".format(mse_calc(yhat_linear, Y_test))\n",
    "print \"Ridge Test MSE: {:.3f}\".format(mse_calc(yhat_ridge, Y_test))\n",
    "print \"LASSO Test MSE: {:.3f}\".format(mse_calc(yhat_lasso, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "The best $\\lambda$ based on the LASSO formula below is ```4409.126```\n",
    "\n",
    "```\n",
    "LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "```\n",
    "\n",
    "The coefficients from this model are:\n",
    "\n",
    "```\n",
    "AtBat: 0.610\n",
    "PutOuts: 0.323\n",
    "CRBI: 0.246\n",
    "CRuns: 0.225\n",
    "CHits: 0.081\n",
    "Hits: 0.000\n",
    "HmRun: 0.000\n",
    "Runs: 0.000\n",
    "RBI: 0.000\n",
    "Walks: 0.000\n",
    "Years: -0.000\n",
    "CAtBat: 0.000\n",
    "CHmRun: 0.000\n",
    "CWalks: 0.000\n",
    "Assists: -0.000\n",
    "Errors: -0.000\n",
    "```\n",
    "We noticed that only 5 of the 16 coefficients had values not equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso_cv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "\n",
    "lasso_cv.fit(X_train, Y_train)\n",
    "best_lambda_lasso_4 = lasso_cv.alpha_\n",
    "\n",
    "coef_values = lasso_cv.coef_\n",
    "features = X_train.columns.values\n",
    "\n",
    "features_coef = [(features[i], coef_values[i]) for i in range(len(features))]\n",
    "\n",
    "print \"Best lambda, LASSO: {:.3f}\".format(best_lambda_lasso_4), '\\n'\n",
    "\n",
    "for tup in sorted(features_coef, key=lambda tup: tup[1], reverse=True):\n",
    "    print \"{}: {:.3f}\".format(tup[0], tup[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Shiny app using R (10 points)\n",
    "\n",
    "In this problem, you'll build a Shiny application. Shiny is an R\n",
    "package which lets you publish web applications from R easily. For\n",
    "more information on Shiny, see http://shiny.rstudio.com. The problem\n",
    "statement is as follows:\n",
    "\n",
    "We are going to use the \"WorldPhones\" dataset available in\n",
    "\"datasets\" package. This dataset shows the number of\n",
    "telephones (in thousands) in various regions of the world in\n",
    "different years. The dataset will be loaded into a variable named\n",
    "\"WorldPhones\" once you include the datasets\n",
    "package (library(datasets)). Your goal is to build a Shiny\n",
    "app which allows the user to visualize the distribution of the\n",
    "number of telephones by region and by year (using bar graph). The\n",
    "requirements are as follows:\n",
    "\n",
    "\n",
    "1. You will give the user the option to choose between \"Region\" and\n",
    "\"Year\". Use [check-box](http://shiny.rstudio.com/reference/shiny/latest/checkboxGroupInput.html) to get the user option. The default\n",
    "option should be \"Region\".\n",
    "2. You will also give the user the ability to choose between different regions and years. (Hint: You can make use of drop-down lists)\n",
    "3. Plot a bargraph of the feature chosen by the user. For example, if the user\n",
    "selects \"Region\" using the check-box, and then selects\n",
    "\"Asia\" from the drop-down list of \"Region\",\n",
    "you need to plot a bar graph showing the number of telephones in\n",
    "Asia in various years. Similarly, if the user selects\n",
    "\"Year\" using the check-box, and then selects\n",
    "\"1951\" from the drop-down list of \"Year\", you\n",
    "need to plot a bar graph showing the number of telephones in 1951 in\n",
    "various regions. Note that if the user selects both Region and Year\n",
    "(using the check-boxes), the app will work as if only the Region has\n",
    "been selected.\n",
    "\n",
    "\n",
    "We have made available sample screenshots of our Shiny app that\n",
    "supports the above requirements, namely $shiny1.png$ and\n",
    "$shiny2.png$ (available on canvas). Your interface should look\n",
    "similar to the screenshots.\n",
    "\n",
    "\n",
    "The tutorials listed below should provide you the needed background\n",
    "to solve this problem:\n",
    "\n",
    "1. http://shiny.rstudio.com/tutorial/lesson1\n",
    "2. http://shiny.rstudio.com/gallery/\n",
    "3. http://shiny.rstudio.com/reference/shiny/latest/checkboxGroupInput.html\n",
    "\n",
    "You can submit the code and results via a PDF or other format. Just please make a reference to it in your notebook. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
